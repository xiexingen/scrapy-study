1. 通过conda安装  

conda install -c conda-forge scrapy

2. 初始化项目  
scrapy startproject [项目名]

3. 执行

scrapy crawl [爬虫名]

4. 输出到json文件  
`cd [爬虫根目录]`  
scrapy crawl [爬虫名] -o [xx.json]

5. 跟crawlab关联
task_id的值为:os.environ.get('CRAWLAB_TASK_ID') 即可


> 资料
https://zhuanlan.zhihu.com/p/82641501  

* `response.xpath('//a[contains(@href, "image")]')`
* `response.xpath('td[1]/a/@id').get().strip(),`
* `response.xpath('td[2]/a/@id').extract_first(default='').strip()` : 处理为None的不报错
* `response.xpath('td[2]/a/@id').extract()`
* `response.xpath('//a[contains(@href, "image")]/@href').extract()` : 所有a标签href属性值含有image的href属性值列表
* `response.xpath('//a[contains(@href, "image")]/text()').re(r'Name:\s*(.*)')` : 匹配括号里面的所有
* `response.xpath('//a[contains(@href, "image")]/text()').re_first(r'Name:\s*(.*)')` : 匹配正则括号内部的第一条
* `tr.xpath('.//p')` : 相对路径
* `sel.xpath("//a[contains(., 'Next Page')]").extract()`: 避免使用.//text(),直接使用.
* `//node[1]`: 选择所有位于第一个子节点位置的node节点
* `(//node)[1]`: 选择所有的node节点，然后返回结果中的第一个node节点
* `response.xpath('//table[@class="Tab"]//tr[position()>1]')` 返回第二行开始的所有行
* `response.xpath(".//div[@class='STYLE2'][contains(., '第一例受试者入组日期')]/following-sibling::table[1]//td/text()")` 含有指定文字的后面的兄弟元素

* `response.css('base::attr(href)').extract()` : css选择器获取base的href属性
* `response.css('a[href*=image]::attr(href)').extract()`